{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5db4887-a80c-433e-8e59-3bb891791a0c",
   "metadata": {},
   "source": [
    "# Coursework file containing code for sorting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1502e2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code takes a sample of the kaggle dataset.\n",
    "# The rationale is to manipulate the labels first and then select the matching images\n",
    "# Original images are in /INM705/705 Coursework/datasets/data_tsinghua/images\n",
    "# Original labels are in /INM705/705 Coursework/datasets/data_tsinghua/labels\n",
    "#\n",
    "# File operations learnt from Ceder, N. (2018) The Quick Python Handbook, 3rd Ed. Shelter Island: Manning\n",
    "\n",
    "# Setting up the data for the model and setting up the model adapted from https://towardsdatascience.com/the-practical-guide-for-object-detection-with-yolov5-algorithm-74c04aac4843"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b363901b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/charlesciumei/Dropbox/!! python notebooks/INM705 DL and image analysis/Cyclist Detection YOLOv5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "import shutil\n",
    "import torch\n",
    "from IPython import display\n",
    "from IPython.display import clear_output\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "\n",
    "# NB Code assumes download of dataset from Kaggle as zip file (see report)\n",
    "# Expand zip file inside \"/Cyclist Detection YOLOv5/data\" \n",
    "\n",
    "os.chdir(\"/Users/charlesciumei/Dropbox/!! python notebooks/INM705 DL and image analysis/Cyclist Detection YOLOv5\") \n",
    "# change this to correct pathname on file system\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "original_dataset_pathname = \"data/data_tsinghua\"\n",
    "\n",
    "# Make new directories to hold sample of data if they don't exist\n",
    "\n",
    "if os.path.exists('data/sample_data/labels_zero'):\n",
    "    pass\n",
    "else:\n",
    "    os.makedirs('data/sample_data/labels_zero')\n",
    "\n",
    "if os.path.exists('data/sample_data/labels_sample'):\n",
    "    pass\n",
    "else:\n",
    "    os.makedirs('data/sample_data/labels_sample')\n",
    "    \n",
    "if os.path.exists('data/sample_data/images_sample'):\n",
    "    pass\n",
    "else:\n",
    "    os.makedirs('data/sample_data/images_sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33b1f34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proportion of the dataset to be sampled:\n",
    "#\n",
    "# The kaggle dataset contains 13,672 images including 1,621 empty images for background, i.e. approximately 12%.\n",
    "# https://github.com/ultralytics/yolov5/issues/5851 recommends â‰¥ 1500 images per class and 0-10% background images to reduce false positives.\n",
    "# After experimenting with different proportions, 10% for overall sample and 20% background was used.\n",
    "# The main reason for this was getting a small enough sample to train. \n",
    "\n",
    "sample_prop = 0.10\n",
    "background_prop = 0.20\n",
    "\n",
    "def get_random_files(source_dir, target_dir, ext, prop):\n",
    "    # moves a random selection of files of the specified type to 'labels_sample'\n",
    "    sample_number = round(len(os.listdir(source_dir)) * prop)  # convert percentage of files to sample to a number\n",
    "    for x in range(sample_number):\n",
    "        file_list = os.listdir(source_dir)\n",
    "        rand = random.randint(0, len(file_list) - 1)\n",
    "        # match only files with specified extension and move them by renaming to target directory\n",
    "        if file_list[rand].split('.')[1] == ext:\n",
    "            os.rename(f'{source_dir}/{file_list[rand]}',\n",
    "                      f'{target_dir}/{file_list[rand]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc5981ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1623 files selected\n"
     ]
    }
   ],
   "source": [
    "# Search the original labels directory and move any labels for images with no cycles to a new directory\n",
    "\n",
    "with os.scandir(f'{original_dataset_pathname}/labels') as my_dir:\n",
    "    for file in my_dir:\n",
    "        if os.path.getsize(file) == 0:\n",
    "            os.rename(f'{original_dataset_pathname}/labels/{file.name}',\n",
    "                      f'data/sample_data/labels_zero/{file.name}')\n",
    "\n",
    "print(f'{len(os.listdir(\"data/sample_data/labels_zero\"))} files selected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29e30971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1205 label files selected\n"
     ]
    }
   ],
   "source": [
    "# Randomly select the specified proportion of labels and move to 'labels_sample'\n",
    "\n",
    "get_random_files(f'{original_dataset_pathname}/labels', 'data/sample_data/labels_sample', \"txt\", sample_prop)\n",
    "\n",
    "labels_chosen = len(os.listdir(\"data/sample_data/labels_sample\"))\n",
    "\n",
    "print(f'{len(os.listdir(\"data/sample_data/labels_sample\"))} label files selected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f22d0096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325 zero label files selected\n",
      "1530 total label files selected\n"
     ]
    }
   ],
   "source": [
    "# Randomly select the specified proportion of zero labels and move to the sample labels directory\n",
    "\n",
    "get_random_files('data/sample_data/labels_zero', 'data/sample_data/labels_sample', \"txt\", background_prop)\n",
    "\n",
    "print(f'{len(os.listdir(\"data/sample_data/labels_sample\")) - labels_chosen} zero label files selected')\n",
    "\n",
    "print(f'{len(os.listdir(\"data/sample_data/labels_sample\"))} total label files selected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25588364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1205 total image files selected\n"
     ]
    }
   ],
   "source": [
    "# select a sample of the images by matching against the sampled label filenames\n",
    "\n",
    "with os.scandir('data/sample_data/labels_sample') as my_dir:\n",
    "    for file in my_dir:\n",
    "        if file.name.endswith('.txt'):\n",
    "            image_name = file.name[:-4] + '.jpg'\n",
    "            image_path = os.path.join(original_dataset_pathname, 'images', image_name)\n",
    "            if os.path.exists(image_path):\n",
    "                new_image_path = os.path.join('data/sample_data/images_sample', image_name)\n",
    "                shutil.move(image_path, new_image_path)\n",
    "\n",
    "print(f'{len(os.listdir(\"data/sample_data/images_sample\"))} total image files selected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77a6a7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/charlesciumei/Dropbox/!! python notebooks/INM705 DL and image analysis/Cyclist Detection YOLOv5\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c74aad6f-5d44-4201-b10a-c947011aac6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directories in the structure required by the YOLO model\n",
    "\n",
    "def create_data_directories(data_name):\n",
    "    Path(f\"./data/{data_name}/images/train\").mkdir(parents=True, exist_ok=True)\n",
    "    Path(f\"./data/{data_name}/images/valid\").mkdir(parents=True, exist_ok=True)\n",
    "    Path(f\"./data/{data_name}/images/test\").mkdir(parents=True, exist_ok=True)\n",
    "    Path(f\"./data/{data_name}/labels/train\").mkdir(parents=True, exist_ok=True)\n",
    "    Path(f\"./data/{data_name}/labels/valid\").mkdir(parents=True, exist_ok=True)\n",
    "    Path(f\"./data/{data_name}/labels/test\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    \n",
    "create_data_directories('cyclist_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6be65dd-da5a-4779-bca8-5e7be56f695b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/charlesciumei/Dropbox/!! python notebooks/INM705 DL and image analysis/Cyclist Detection YOLOv5\n"
     ]
    }
   ],
   "source": [
    "# Populate the train, validation and test folders from the sampled data\n",
    "# Split as follows:\n",
    "# 60% train = 918\n",
    "# 20% validation = 306\n",
    "# 20% test = 306 files\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "def move_files(source_dir, target_dir, ext, prop):\n",
    "    # moves a selection of files of the specified type to target directory\n",
    "    file_list = os.listdir(source_dir)\n",
    "    sample_number = round(len(os.listdir(source_dir)) * prop)  # convert percentage of files to sample to a number\n",
    "    for x in range(sample_number):\n",
    "        # match only files with specified extension and move them by renaming to target directory\n",
    "        if file_list[x].split('.')[1] == ext:\n",
    "            os.rename(f'{source_dir}/{file_list[x]}',\n",
    "                      f'{target_dir}/{file_list[x]}')\n",
    "\n",
    "\n",
    "# Moving the files. Logic as follows. For training data, proportion is 0.6 because 60% is used for training.\n",
    "# 40% is left. Therefore proportion for Validation is set to 0.5 brackets being 20% of the dataset.\n",
    "# Which leaves 20% for testing: proportion is 1.0 because all the remaining data is moved to test directory.\n",
    "\n",
    "# 06-08-2023 this put this in the wrong directory because the structure is created in the wrong folder\n",
    "                \n",
    "move_files('data/sample_data/labels_sample', 'data/cyclist_data/labels/train', \"txt\", 0.6)\n",
    "move_files('data/sample_data/labels_sample', 'data/cyclist_data/labels/valid', \"txt\", 0.5)\n",
    "move_files('data/sample_data/labels_sample', 'data/cyclist_data/labels/test', \"txt\", 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67508464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move the training images by matching to labels\n",
    "\n",
    "with os.scandir('data/cyclist_data/labels/train') as my_dir:\n",
    "    for file in my_dir:\n",
    "        if file.name.endswith('.txt'):\n",
    "            image_name = file.name[:-4] + '.jpg'\n",
    "            image_path = os.path.join('data/sample_data/images_sample', image_name)\n",
    "            if os.path.exists(image_path):\n",
    "                new_image_path = os.path.join('data/cyclist_data/images/train', image_name)\n",
    "                shutil.move(image_path, new_image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5c57403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move the validation images by matching to labels\n",
    "\n",
    "with os.scandir('data/cyclist_data/labels/valid') as my_dir:\n",
    "    for file in my_dir:\n",
    "        if file.name.endswith('.txt'):\n",
    "            image_name = file.name[:-4] + '.jpg'\n",
    "            image_path = os.path.join('data/sample_data/images_sample', image_name)\n",
    "            if os.path.exists(image_path):\n",
    "                new_image_path = os.path.join('data/cyclist_data/images/valid', image_name)\n",
    "                shutil.move(image_path, new_image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf8bab52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move the test images by matching to labels\n",
    "\n",
    "with os.scandir('data/cyclist_data/labels/test') as my_dir:\n",
    "    for file in my_dir:\n",
    "        if file.name.endswith('.txt'):\n",
    "            image_name = file.name[:-4] + '.jpg'\n",
    "            image_path = os.path.join('data/sample_data/images_sample', image_name)\n",
    "            if os.path.exists(image_path):\n",
    "                new_image_path = os.path.join('data/cyclist_data/images/test', image_name)\n",
    "                shutil.move(image_path, new_image_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923df1a6",
   "metadata": {},
   "source": [
    "**NB below this is the original code for training the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f72d0a-1832-4bc8-a775-026be4805b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install YOLO v5\n",
    "\n",
    "%t+https://github.com/ultralytics/yolov5.git\n",
    "%cd /users/addj212/INM705/INM705 Coursework/yolov5\n",
    "%pip install -r requirements.txt\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e9acf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%git clone https://github.com/ultralytics/yolov5  # clone\n",
    "%cd yolov5\n",
    "%pip install -r requirements.txt  # install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6655167f-f57c-4da3-92a5-785273c5483f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/charlesciumei/Dropbox/!! python notebooks/INM705 DL and image analysis\n",
      "/Users/charlesciumei/Dropbox/!! python notebooks/INM705 DL and image analysis/yolov5\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "# os.chdir(\"/users/addj212/INM705/705 Coursework/yolov5\")\n",
    "os.chdir(\"/Users/charlesciumei/Dropbox/!! python notebooks/INM705 DL and image analysis/yolov5\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77602aeb-e074-4c51-96e2-0017371065c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5n6.pt, cfg=, data=data/cyclists.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=10, batch_size=8, imgsz=1280, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[12], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 âœ…\n",
      "fatal: cannot change to '/Users/charlesciumei/Dropbox/!!': No such file or directory\n",
      "YOLOv5 ðŸš€ 2023-8-5 Python-3.9.7 torch-2.0.1 CPU\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 ðŸš€ runs in Comet\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5n6.pt to yolov5n6.pt...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.86M/6.86M [00:00<00:00, 27.9MB/s]\n",
      "\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      1760  models.common.Conv                      [3, 16, 6, 2, 2]              \n",
      "  1                -1  1      4672  models.common.Conv                      [16, 32, 3, 2]                \n",
      "  2                -1  1      4800  models.common.C3                        [32, 32, 1]                   \n",
      "  3                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  4                -1  2     29184  models.common.C3                        [64, 64, 2]                   \n",
      "  5                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  6                -1  3    156928  models.common.C3                        [128, 128, 3]                 \n",
      "  7                -1  1    221568  models.common.Conv                      [128, 192, 3, 2]              \n",
      "  8                -1  1    167040  models.common.C3                        [192, 192, 1]                 \n",
      "  9                -1  1    442880  models.common.Conv                      [192, 256, 3, 2]              \n",
      " 10                -1  1    296448  models.common.C3                        [256, 256, 1]                 \n",
      " 11                -1  1    164608  models.common.SPPF                      [256, 256, 5]                 \n",
      " 12                -1  1     49536  models.common.Conv                      [256, 192, 1, 1]              \n",
      " 13                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 14           [-1, 8]  1         0  models.common.Concat                    [1]                           \n",
      " 15                -1  1    203904  models.common.C3                        [384, 192, 1, False]          \n",
      " 16                -1  1     24832  models.common.Conv                      [192, 128, 1, 1]              \n",
      " 17                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 18           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 19                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 20                -1  1      8320  models.common.Conv                      [128, 64, 1, 1]               \n",
      " 21                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 22           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1     22912  models.common.C3                        [128, 64, 1, False]           \n",
      " 24                -1  1     36992  models.common.Conv                      [64, 64, 3, 2]                \n",
      " 25          [-1, 20]  1         0  models.common.Concat                    [1]                           \n",
      " 26                -1  1     74496  models.common.C3                        [128, 128, 1, False]          \n",
      " 27                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 28          [-1, 16]  1         0  models.common.Concat                    [1]                           \n",
      " 29                -1  1    179328  models.common.C3                        [256, 192, 1, False]          \n",
      " 30                -1  1    332160  models.common.Conv                      [192, 192, 3, 2]              \n",
      " 31          [-1, 12]  1         0  models.common.Concat                    [1]                           \n",
      " 32                -1  1    329216  models.common.C3                        [384, 256, 1, False]          \n",
      " 33  [23, 26, 29, 32]  1     11592  models.yolo.Detect                      [1, [[19, 27, 44, 40, 38, 94], [96, 68, 86, 152, 180, 137], [140, 301, 303, 264, 238, 542], [436, 615, 739, 380, 925, 792]], [64, 128, 192, 256]]\n",
      "Model summary: 281 layers, 3094312 parameters, 3094312 gradients, 4.3 GFLOPs\n",
      "\n",
      "Transferred 451/459 items from yolov5n6.pt\n",
      "freezing model.0.conv.weight\n",
      "freezing model.0.bn.weight\n",
      "freezing model.0.bn.bias\n",
      "freezing model.1.conv.weight\n",
      "freezing model.1.bn.weight\n",
      "freezing model.1.bn.bias\n",
      "freezing model.2.cv1.conv.weight\n",
      "freezing model.2.cv1.bn.weight\n",
      "freezing model.2.cv1.bn.bias\n",
      "freezing model.2.cv2.conv.weight\n",
      "freezing model.2.cv2.bn.weight\n",
      "freezing model.2.cv2.bn.bias\n",
      "freezing model.2.cv3.conv.weight\n",
      "freezing model.2.cv3.bn.weight\n",
      "freezing model.2.cv3.bn.bias\n",
      "freezing model.2.m.0.cv1.conv.weight\n",
      "freezing model.2.m.0.cv1.bn.weight\n",
      "freezing model.2.m.0.cv1.bn.bias\n",
      "freezing model.2.m.0.cv2.conv.weight\n",
      "freezing model.2.m.0.cv2.bn.weight\n",
      "freezing model.2.m.0.cv2.bn.bias\n",
      "freezing model.3.conv.weight\n",
      "freezing model.3.bn.weight\n",
      "freezing model.3.bn.bias\n",
      "freezing model.4.cv1.conv.weight\n",
      "freezing model.4.cv1.bn.weight\n",
      "freezing model.4.cv1.bn.bias\n",
      "freezing model.4.cv2.conv.weight\n",
      "freezing model.4.cv2.bn.weight\n",
      "freezing model.4.cv2.bn.bias\n",
      "freezing model.4.cv3.conv.weight\n",
      "freezing model.4.cv3.bn.weight\n",
      "freezing model.4.cv3.bn.bias\n",
      "freezing model.4.m.0.cv1.conv.weight\n",
      "freezing model.4.m.0.cv1.bn.weight\n",
      "freezing model.4.m.0.cv1.bn.bias\n",
      "freezing model.4.m.0.cv2.conv.weight\n",
      "freezing model.4.m.0.cv2.bn.weight\n",
      "freezing model.4.m.0.cv2.bn.bias\n",
      "freezing model.4.m.1.cv1.conv.weight\n",
      "freezing model.4.m.1.cv1.bn.weight\n",
      "freezing model.4.m.1.cv1.bn.bias\n",
      "freezing model.4.m.1.cv2.conv.weight\n",
      "freezing model.4.m.1.cv2.bn.weight\n",
      "freezing model.4.m.1.cv2.bn.bias\n",
      "freezing model.5.conv.weight\n",
      "freezing model.5.bn.weight\n",
      "freezing model.5.bn.bias\n",
      "freezing model.6.cv1.conv.weight\n",
      "freezing model.6.cv1.bn.weight\n",
      "freezing model.6.cv1.bn.bias\n",
      "freezing model.6.cv2.conv.weight\n",
      "freezing model.6.cv2.bn.weight\n",
      "freezing model.6.cv2.bn.bias\n",
      "freezing model.6.cv3.conv.weight\n",
      "freezing model.6.cv3.bn.weight\n",
      "freezing model.6.cv3.bn.bias\n",
      "freezing model.6.m.0.cv1.conv.weight\n",
      "freezing model.6.m.0.cv1.bn.weight\n",
      "freezing model.6.m.0.cv1.bn.bias\n",
      "freezing model.6.m.0.cv2.conv.weight\n",
      "freezing model.6.m.0.cv2.bn.weight\n",
      "freezing model.6.m.0.cv2.bn.bias\n",
      "freezing model.6.m.1.cv1.conv.weight\n",
      "freezing model.6.m.1.cv1.bn.weight\n",
      "freezing model.6.m.1.cv1.bn.bias\n",
      "freezing model.6.m.1.cv2.conv.weight\n",
      "freezing model.6.m.1.cv2.bn.weight\n",
      "freezing model.6.m.1.cv2.bn.bias\n",
      "freezing model.6.m.2.cv1.conv.weight\n",
      "freezing model.6.m.2.cv1.bn.weight\n",
      "freezing model.6.m.2.cv1.bn.bias\n",
      "freezing model.6.m.2.cv2.conv.weight\n",
      "freezing model.6.m.2.cv2.bn.weight\n",
      "freezing model.6.m.2.cv2.bn.bias\n",
      "freezing model.7.conv.weight\n",
      "freezing model.7.bn.weight\n",
      "freezing model.7.bn.bias\n",
      "freezing model.8.cv1.conv.weight\n",
      "freezing model.8.cv1.bn.weight\n",
      "freezing model.8.cv1.bn.bias\n",
      "freezing model.8.cv2.conv.weight\n",
      "freezing model.8.cv2.bn.weight\n",
      "freezing model.8.cv2.bn.bias\n",
      "freezing model.8.cv3.conv.weight\n",
      "freezing model.8.cv3.bn.weight\n",
      "freezing model.8.cv3.bn.bias\n",
      "freezing model.8.m.0.cv1.conv.weight\n",
      "freezing model.8.m.0.cv1.bn.weight\n",
      "freezing model.8.m.0.cv1.bn.bias\n",
      "freezing model.8.m.0.cv2.conv.weight\n",
      "freezing model.8.m.0.cv2.bn.weight\n",
      "freezing model.8.m.0.cv2.bn.bias\n",
      "freezing model.9.conv.weight\n",
      "freezing model.9.bn.weight\n",
      "freezing model.9.bn.bias\n",
      "freezing model.10.cv1.conv.weight\n",
      "freezing model.10.cv1.bn.weight\n",
      "freezing model.10.cv1.bn.bias\n",
      "freezing model.10.cv2.conv.weight\n",
      "freezing model.10.cv2.bn.weight\n",
      "freezing model.10.cv2.bn.bias\n",
      "freezing model.10.cv3.conv.weight\n",
      "freezing model.10.cv3.bn.weight\n",
      "freezing model.10.cv3.bn.bias\n",
      "freezing model.10.m.0.cv1.conv.weight\n",
      "freezing model.10.m.0.cv1.bn.weight\n",
      "freezing model.10.m.0.cv1.bn.bias\n",
      "freezing model.10.m.0.cv2.conv.weight\n",
      "freezing model.10.m.0.cv2.bn.weight\n",
      "freezing model.10.m.0.cv2.bn.bias\n",
      "freezing model.11.cv1.conv.weight\n",
      "freezing model.11.cv1.bn.weight\n",
      "freezing model.11.cv1.bn.bias\n",
      "freezing model.11.cv2.conv.weight\n",
      "freezing model.11.cv2.bn.weight\n",
      "freezing model.11.cv2.bn.bias\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 75 weight(decay=0.0), 79 weight(decay=0.0005), 79 bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/charlesciumei/Dropbox/!! python notebooks/INM705 DL and i\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /Users/charlesciumei/Dropbox/!! python notebooks/INM705 DL and image analysis/datasets/cyclist_data/labels/train.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/charlesciumei/Dropbox/!! python notebooks/INM705 DL and ima\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ No labels found in /Users/charlesciumei/Dropbox/!! python notebooks/INM705 DL and image analysis/datasets/cyclist_data/labels/valid.cache. See https://docs.ultralytics.com/yolov5/tutorials/train_custom_data\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /Users/charlesciumei/Dropbox/!! python notebooks/INM705 DL and image analysis/datasets/cyclist_data/labels/valid.cache\n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m5.07 anchors/target, 0.992 Best Possible Recall (BPR). Current anchors are a good fit to dataset âœ…\n",
      "Plotting labels to runs/train/exp/labels.jpg... \n",
      "Image sizes 1280 train, 1280 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/train/exp\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        0/9         0G    0.08129    0.03654          0          7       1280: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        306          0          0          0          0          0\n",
      "WARNING âš ï¸ no labels found in val set, can not compute metrics without labels\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        1/9         0G    0.05658     0.0188          0          7       1280: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        306          0          0          0          0          0\n",
      "WARNING âš ï¸ no labels found in val set, can not compute metrics without labels\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        2/9         0G    0.05244    0.01603          0          9       1280: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        306          0          0          0          0          0\n",
      "WARNING âš ï¸ no labels found in val set, can not compute metrics without labels\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        3/9         0G    0.04493     0.0147          0         18       1280: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        306          0          0          0          0          0\n",
      "WARNING âš ï¸ no labels found in val set, can not compute metrics without labels\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        4/9         0G     0.0406    0.01381          0         10       1280: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        306          0          0          0          0          0\n",
      "WARNING âš ï¸ no labels found in val set, can not compute metrics without labels\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        5/9         0G    0.03513    0.01269          0         19       1280: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        306          0          0          0          0          0\n",
      "WARNING âš ï¸ no labels found in val set, can not compute metrics without labels\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        6/9         0G    0.03104    0.01219          0         22       1280: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        306          0          0          0          0          0\n",
      "WARNING âš ï¸ no labels found in val set, can not compute metrics without labels\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        7/9         0G    0.02884    0.01116          0         12       1280: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        306          0          0          0          0          0\n",
      "WARNING âš ï¸ no labels found in val set, can not compute metrics without labels\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        8/9         0G    0.02666    0.01107          0         11       1280: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        306          0          0          0          0          0\n",
      "WARNING âš ï¸ no labels found in val set, can not compute metrics without labels\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        9/9         0G    0.02359    0.01134          0         13       1280: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        306          0          0          0          0          0\n",
      "WARNING âš ï¸ no labels found in val set, can not compute metrics without labels\n",
      "\n",
      "10 epochs completed in 1.034 hours.\n",
      "Optimizer stripped from runs/train/exp/weights/last.pt, 6.9MB\n",
      "Optimizer stripped from runs/train/exp/weights/best.pt, 6.9MB\n",
      "\n",
      "Validating runs/train/exp/weights/best.pt...\n",
      "Fusing layers... \n",
      "Model summary: 206 layers, 3087256 parameters, 0 gradients, 4.2 GFLOPs\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        306          0          0          0          0          0\n",
      "WARNING âš ï¸ no labels found in val set, can not compute metrics without labels\n",
      "Results saved to \u001b[1mruns/train/exp\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "\n",
    "# Output saved to 'Runs, folder\n",
    "\n",
    "!python train.py --data data/cyclists.yaml --img 1280 --batch 8 --epochs 10 --weights yolov5n6.pt --freeze 12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d17fb27-cfb4-49fa-b537-5cfbecda1ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=runs/train/exp13/weights/best.pt, cfg=, data=data/cyclists.yaml, hyp=data/hyps/hyp.VOC.yaml, epochs=100, batch_size=8, imgsz=1280, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs_cyclists, name=fine-tuning, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 âœ…\n",
      "fatal: cannot change to '/Users/charlesciumei/Dropbox/!!': No such file or directory\n",
      "YOLOv5 ðŸš€ 2023-8-5 Python-3.9.7 torch-2.0.1 CPU\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.00334, lrf=0.15135, momentum=0.74832, weight_decay=0.00025, warmup_epochs=3.3835, warmup_momentum=0.59462, warmup_bias_lr=0.18657, box=0.02, cls=0.21638, cls_pw=0.5, obj=0.51728, obj_pw=0.67198, iou_t=0.2, anchor_t=3.3744, fl_gamma=0.0, hsv_h=0.01041, hsv_s=0.54703, hsv_v=0.27739, degrees=0.0, translate=0.04591, scale=0.75544, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=0.85834, mixup=0.04266, copy_paste=0.0, anchors=3.412\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 ðŸš€ runs in Comet\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs_cyclists', view at http://localhost:6006/\n",
      "Downloading https://ultralytics.com/assets/Arial.ttf to /Users/charlesciumei/Library/Application Support/Ultralytics/Arial.ttf...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 755k/755k [00:00<00:00, 28.9MB/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/charlesciumei/Dropbox/!! python notebooks/INM705 DL and image analysis/yolov5/train.py\", line 647, in <module>\n",
      "    main(opt)\n",
      "  File \"/Users/charlesciumei/Dropbox/!! python notebooks/INM705 DL and image analysis/yolov5/train.py\", line 536, in main\n",
      "    train(opt.hyp, opt, device, callbacks)\n",
      "  File \"/Users/charlesciumei/Dropbox/!! python notebooks/INM705 DL and image analysis/yolov5/train.py\", line 129, in train\n",
      "    ckpt = torch.load(weights, map_location='cpu')  # load checkpoint to CPU to avoid CUDA memory leak\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/torch/serialization.py\", line 791, in load\n",
      "    with _open_file_like(f, 'rb') as opened_file:\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/torch/serialization.py\", line 271, in _open_file_like\n",
      "    return _open_file(name_or_buffer, mode)\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/torch/serialization.py\", line 252, in __init__\n",
      "    super().__init__(open(name, mode))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'runs/train/exp13/weights/best.pt'\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning\n",
    "\n",
    "!python train.py --hyp 'data/hyps/hyp.VOC.yaml' --img 1280 --batch 8 --epochs 100 --data 'data/cyclists.yaml' --weights 'runs/train/exp13/weights/best.pt' --project 'runs_cyclists' --name 'fine-tuning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12c0dd84-ac07-4a3f-ae09-5c796a4fda63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mdata=data/cyclists.yaml, weights=['runs_cyclists/fine-tuning2/weights/best.pt'], batch_size=8, imgsz=1280, conf_thres=0.001, iou_thres=0.6, max_det=300, task=test, device=, workers=8, single_cls=False, augment=True, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs_cyclists, name=Test, exist_ok=False, half=False, dnn=False\n",
      "Unknown option: -C\n",
      "usage: git [--version] [--help] [-c name=value]\n",
      "           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n",
      "           [-p|--paginate|--no-pager] [--no-replace-objects] [--bare]\n",
      "           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n",
      "           <command> [<args>]\n",
      "YOLOv5 ðŸš€ 2023-4-10 Python-3.9.5 torch-1.10.0 CUDA:0 (A100-PCIE-80GB, 81251MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 206 layers, 12308200 parameters, 0 gradients, 16.1 GFLOPs\n",
      "\u001b[34m\u001b[1mtest: \u001b[0mScanning /users/addj212/INM705/705 Coursework/datasets/cyclist_data/labels\u001b[0m\n",
      "\u001b[34m\u001b[1mtest: \u001b[0mNew cache created: /users/addj212/INM705/705 Coursework/datasets/cyclist_data/labels/test.cache\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        394        110       0.15      0.691      0.139      0.112\n",
      "Speed: 3.2ms pre-process, 24.1ms inference, 8.8ms NMS per image at shape (8, 3, 1280, 1280)\n",
      "Results saved to \u001b[1mruns_cyclists/Test\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Testing the model against unseen data\n",
    "\n",
    "!python val.py --img 1280 --batch 8 --data 'data/cyclists.yaml' --weights 'runs_cyclists/fine-tuning2/weights/best.pt' --task test --project 'runs_cyclists' --name 'Test' --augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d878edc-1c99-4f68-a569-f46b2c567900",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a4d2d9-17ea-436b-b513-256a665b7dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s6.pt, cfg=, data=data/cyclists.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=300, batch_size=8, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[12], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0mskipping check (offline), for updates see https://github.com/ultralytics/yolov5\n",
      "Unknown option: -C\n",
      "usage: git [--version] [--help] [-c name=value]\n",
      "           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n",
      "           [-p|--paginate|--no-pager] [--no-replace-objects] [--bare]\n",
      "           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n",
      "           <command> [<args>]\n",
      "YOLOv5 ðŸš€ 2023-4-10 Python-3.9.5 torch-1.10.0 CUDA:0 (A100-PCIE-80GB, 81251MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 ðŸš€ in ClearML\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 ðŸš€ runs in Comet\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1    885504  models.common.Conv                      [256, 384, 3, 2]              \n",
      "  8                -1  1    665856  models.common.C3                        [384, 384, 1]                 \n",
      "  9                -1  1   1770496  models.common.Conv                      [384, 512, 3, 2]              \n",
      " 10                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      " 11                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 12                -1  1    197376  models.common.Conv                      [512, 384, 1, 1]              \n",
      " 13                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 14           [-1, 8]  1         0  models.common.Concat                    [1]                           \n",
      " 15                -1  1    813312  models.common.C3                        [768, 384, 1, False]          \n",
      " 16                -1  1     98816  models.common.Conv                      [384, 256, 1, 1]              \n",
      " 17                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 18           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 19                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 20                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 21                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 22           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 24                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 25          [-1, 20]  1         0  models.common.Concat                    [1]                           \n",
      " 26                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 27                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 28          [-1, 16]  1         0  models.common.Concat                    [1]                           \n",
      " 29                -1  1    715008  models.common.C3                        [512, 384, 1, False]          \n",
      " 30                -1  1   1327872  models.common.Conv                      [384, 384, 3, 2]              \n",
      " 31          [-1, 12]  1         0  models.common.Concat                    [1]                           \n",
      " 32                -1  1   1313792  models.common.C3                        [768, 512, 1, False]          \n",
      " 33  [23, 26, 29, 32]  1     23112  models.yolo.Detect                      [1, [[19, 27, 44, 40, 38, 94], [96, 68, 86, 152, 180, 137], [140, 301, 303, 264, 238, 542], [436, 615, 739, 380, 925, 792]], [128, 256, 384, 512]]\n",
      "Model summary: 281 layers, 12322312 parameters, 12322312 gradients, 16.3 GFLOPs\n",
      "\n",
      "Transferred 451/459 items from yolov5s6.pt\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "freezing model.0.conv.weight\n",
      "freezing model.0.bn.weight\n",
      "freezing model.0.bn.bias\n",
      "freezing model.1.conv.weight\n",
      "freezing model.1.bn.weight\n",
      "freezing model.1.bn.bias\n",
      "freezing model.2.cv1.conv.weight\n",
      "freezing model.2.cv1.bn.weight\n",
      "freezing model.2.cv1.bn.bias\n",
      "freezing model.2.cv2.conv.weight\n",
      "freezing model.2.cv2.bn.weight\n",
      "freezing model.2.cv2.bn.bias\n",
      "freezing model.2.cv3.conv.weight\n",
      "freezing model.2.cv3.bn.weight\n",
      "freezing model.2.cv3.bn.bias\n",
      "freezing model.2.m.0.cv1.conv.weight\n",
      "freezing model.2.m.0.cv1.bn.weight\n",
      "freezing model.2.m.0.cv1.bn.bias\n",
      "freezing model.2.m.0.cv2.conv.weight\n",
      "freezing model.2.m.0.cv2.bn.weight\n",
      "freezing model.2.m.0.cv2.bn.bias\n",
      "freezing model.3.conv.weight\n",
      "freezing model.3.bn.weight\n",
      "freezing model.3.bn.bias\n",
      "freezing model.4.cv1.conv.weight\n",
      "freezing model.4.cv1.bn.weight\n",
      "freezing model.4.cv1.bn.bias\n",
      "freezing model.4.cv2.conv.weight\n",
      "freezing model.4.cv2.bn.weight\n",
      "freezing model.4.cv2.bn.bias\n",
      "freezing model.4.cv3.conv.weight\n",
      "freezing model.4.cv3.bn.weight\n",
      "freezing model.4.cv3.bn.bias\n",
      "freezing model.4.m.0.cv1.conv.weight\n",
      "freezing model.4.m.0.cv1.bn.weight\n",
      "freezing model.4.m.0.cv1.bn.bias\n",
      "freezing model.4.m.0.cv2.conv.weight\n",
      "freezing model.4.m.0.cv2.bn.weight\n",
      "freezing model.4.m.0.cv2.bn.bias\n",
      "freezing model.4.m.1.cv1.conv.weight\n",
      "freezing model.4.m.1.cv1.bn.weight\n",
      "freezing model.4.m.1.cv1.bn.bias\n",
      "freezing model.4.m.1.cv2.conv.weight\n",
      "freezing model.4.m.1.cv2.bn.weight\n",
      "freezing model.4.m.1.cv2.bn.bias\n",
      "freezing model.5.conv.weight\n",
      "freezing model.5.bn.weight\n",
      "freezing model.5.bn.bias\n",
      "freezing model.6.cv1.conv.weight\n",
      "freezing model.6.cv1.bn.weight\n",
      "freezing model.6.cv1.bn.bias\n",
      "freezing model.6.cv2.conv.weight\n",
      "freezing model.6.cv2.bn.weight\n",
      "freezing model.6.cv2.bn.bias\n",
      "freezing model.6.cv3.conv.weight\n",
      "freezing model.6.cv3.bn.weight\n",
      "freezing model.6.cv3.bn.bias\n",
      "freezing model.6.m.0.cv1.conv.weight\n",
      "freezing model.6.m.0.cv1.bn.weight\n",
      "freezing model.6.m.0.cv1.bn.bias\n",
      "freezing model.6.m.0.cv2.conv.weight\n",
      "freezing model.6.m.0.cv2.bn.weight\n",
      "freezing model.6.m.0.cv2.bn.bias\n",
      "freezing model.6.m.1.cv1.conv.weight\n",
      "freezing model.6.m.1.cv1.bn.weight\n",
      "freezing model.6.m.1.cv1.bn.bias\n",
      "freezing model.6.m.1.cv2.conv.weight\n",
      "freezing model.6.m.1.cv2.bn.weight\n",
      "freezing model.6.m.1.cv2.bn.bias\n",
      "freezing model.6.m.2.cv1.conv.weight\n",
      "freezing model.6.m.2.cv1.bn.weight\n",
      "freezing model.6.m.2.cv1.bn.bias\n",
      "freezing model.6.m.2.cv2.conv.weight\n",
      "freezing model.6.m.2.cv2.bn.weight\n",
      "freezing model.6.m.2.cv2.bn.bias\n",
      "freezing model.7.conv.weight\n",
      "freezing model.7.bn.weight\n",
      "freezing model.7.bn.bias\n",
      "freezing model.8.cv1.conv.weight\n",
      "freezing model.8.cv1.bn.weight\n",
      "freezing model.8.cv1.bn.bias\n",
      "freezing model.8.cv2.conv.weight\n",
      "freezing model.8.cv2.bn.weight\n",
      "freezing model.8.cv2.bn.bias\n",
      "freezing model.8.cv3.conv.weight\n",
      "freezing model.8.cv3.bn.weight\n",
      "freezing model.8.cv3.bn.bias\n",
      "freezing model.8.m.0.cv1.conv.weight\n",
      "freezing model.8.m.0.cv1.bn.weight\n",
      "freezing model.8.m.0.cv1.bn.bias\n",
      "freezing model.8.m.0.cv2.conv.weight\n",
      "freezing model.8.m.0.cv2.bn.weight\n",
      "freezing model.8.m.0.cv2.bn.bias\n",
      "freezing model.9.conv.weight\n",
      "freezing model.9.bn.weight\n",
      "freezing model.9.bn.bias\n",
      "freezing model.10.cv1.conv.weight\n",
      "freezing model.10.cv1.bn.weight\n",
      "freezing model.10.cv1.bn.bias\n",
      "freezing model.10.cv2.conv.weight\n",
      "freezing model.10.cv2.bn.weight\n",
      "freezing model.10.cv2.bn.bias\n",
      "freezing model.10.cv3.conv.weight\n",
      "freezing model.10.cv3.bn.weight\n",
      "freezing model.10.cv3.bn.bias\n",
      "freezing model.10.m.0.cv1.conv.weight\n",
      "freezing model.10.m.0.cv1.bn.weight\n",
      "freezing model.10.m.0.cv1.bn.bias\n",
      "freezing model.10.m.0.cv2.conv.weight\n",
      "freezing model.10.m.0.cv2.bn.weight\n",
      "freezing model.10.m.0.cv2.bn.bias\n",
      "freezing model.11.cv1.conv.weight\n",
      "freezing model.11.cv1.bn.weight\n",
      "freezing model.11.cv1.bn.bias\n",
      "freezing model.11.cv2.conv.weight\n",
      "freezing model.11.cv2.bn.weight\n",
      "freezing model.11.cv2.bn.bias\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 75 weight(decay=0.0), 79 weight(decay=0.0005), 79 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /users/addj212/INM705/705 Coursework/datasets/cyclist_data/label\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /users/addj212/INM705/705 Coursework/datasets/cyclist_data/labels/\u001b[0m\n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m3.59 anchors/target, 0.954 Best Possible Recall (BPR). Anchors are a poor fit to dataset âš ï¸, attempting to improve...\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mWARNING âš ï¸ Extremely small objects found: 27 of 1298 labels are <3 pixels in size\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 12 anchors on 1298 points...\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.8642: 100%|â–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.25: 1.0000 best possible recall, 7.46 anchors past thr\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=12, img_size=640, metric_all=0.382/0.864-mean/best, past_thr=0.528-mean: 3,10, 8,22, 12,32, 17,44, 23,61, 46,47, 33,86, 44,115, 55,143, 98,98, 73,189, 130,132\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone âœ… (optional: update model *.yaml to use these anchors in the future)\n",
      "Plotting labels to runs/train/exp14/labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/train/exp14\u001b[0m\n",
      "Starting training for 300 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      0/299     0.895G     0.1082    0.01942          0         22        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        393        113     0.0512      0.172     0.0285    0.00963\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      1/299         1G    0.07606    0.01568          0          6        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        393        113      0.127      0.177     0.0737     0.0216\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      2/299         1G    0.07163    0.01326          0         20        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        393        113      0.104       0.31     0.0713     0.0233\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      3/299         1G    0.06647    0.01308          0         28        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        393        113      0.136      0.283     0.0938     0.0508\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      4/299         1G    0.06085    0.01241          0         12        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        393        113      0.159      0.283      0.118     0.0551\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      5/299         1G    0.05688      0.012          0         11        640:  "
     ]
    }
   ],
   "source": [
    "# Train for 300 epochs with smaller image size set hopefully to speed the training up.\n",
    "\n",
    "# Results per epoch were cut off in browser due to Hyperion timeout, but model did run for full epochs. Results saved to runs/train/exp14. \n",
    "\n",
    "!python train.py --data data/cyclists.yaml --img 640 --batch 8 --epochs 300 --weights yolov5s6.pt --freeze 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a9db83-728d-4937-99ac-cc484e6d61fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
